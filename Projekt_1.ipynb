{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projekt 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpQGC2rl+Kx4BGeAxdVKW0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xvobs/UNIFESP-X-ray-Body-Part-Classifier-Project/blob/main/Projekt_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbMnYxHLpPac"
      },
      "outputs": [],
      "source": [
        "import pydicom as dicom\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "def draw_img_dcm(image_path):\n",
        "    ds = dicom.dcmread(image_path)\n",
        "\n",
        "    pixel_array_numpy = ds.pixel_array\n",
        "\n",
        "    plt.imshow(pixel_array_numpy)\n",
        "\n",
        "input_shape = [128, 128, 3]\n",
        "from skimage.transform import resize\n",
        "\n",
        "def dcm_to_pixel_array(image_path):\n",
        "    ds = dicom.dcmread(image_path)\n",
        "    data = ds.pixel_array\n",
        "    # resize to new size\n",
        "    resized_img = resize(data, (input_shape[0], input_shape[0]), anti_aliasing=True)\n",
        "    return resized_img\n",
        "\n",
        "# specify your image path\n",
        "#image_path = '../input/unifesp-xray-bodypart-classification/train/train/train/1002/1.2.826.0.1.3680043.8.498.66151583660414380258303341865105351564/1.2.826.0.1.3680043.8.498.40285474650218685810081647255379256989/1.2.826.0.1.3680043.8.498.37634180084111795685928688942145555908-c.dcm'\n",
        "#draw_img_dcm(image_path)\n",
        "#image_path = '../input/unifesp-xray-bodypart-classification/train/train/train/1002/1.2.826.0.1.3680043.8.498.66151583660414380258303341865105351564/1.2.826.0.1.3680043.8.498.88520490971705323792465522631523753284/1.2.826.0.1.3680043.8.498.12781870272366327239431992873991536152-c.dcm'\n",
        "#draw_img_dcm(image_path)\n",
        "\n",
        "#Match labels in train.csv - train data\n",
        "#Match labels in sample_submission.csv - test data\n",
        "#SOPInstanceUID - Each SOPInstanceUID corresponds to a unique image.\n",
        "#Target - In the training data, this represents the labels assigned to each sample.\n",
        "\n",
        "##THERE CAN BE MULTIPLE CLASSES IN ONE PICTURE\n",
        "\n",
        "#Lables:\n",
        "#    Abdomen = 0\n",
        "#    Ankle = 1\n",
        "#    Cervical Spine = 2\n",
        "#    Chest = 3\n",
        "#    Clavicles = 4\n",
        "#    Elbow = 5\n",
        "#    Feet = 6\n",
        "#    Finger = 7\n",
        "#    Forearm = 8\n",
        "#    Hand = 9\n",
        "#    Hip = 10\n",
        "#    Knee = 11\n",
        "#    Lower Leg = 12\n",
        "#    Lumbar Spine = 13\n",
        "#    Others = 14\n",
        "#    Pelvis = 15\n",
        "#    Shoulder = 16\n",
        "#    Sinus = 17\n",
        "#    Skull = 18\n",
        "#    Thigh = 19\n",
        "#    Thoracic Spine = 20\n",
        "#    Wrist = 21\n",
        "\n",
        "def extract_name_from_path(path):\n",
        "    path = path.replace('../input/unifesp-xray-bodypart-classification/train/train/train/*/*/*/',' ')\n",
        "    name = path.split(\"-\")[-2].split(\"/\")[-1]\n",
        "    return name\n",
        "\n",
        "#load data\n",
        "import glob\n",
        "\n",
        "temp_data_paths = glob.glob('../input/unifesp-xray-bodypart-classification/train/train/train/*/*/*/*.dcm')\n",
        "\n",
        "#3 columns to later assign the label\n",
        "train_data = [[0 for col in range(3)] for row in range(len(temp_data_paths))]\n",
        "\n",
        "#convert dcms to pixel arrays and save the file name in 2d array\n",
        "i = 0\n",
        "for path in temp_data_paths:\n",
        "    train_data[i][0] = dcm_to_pixel_array(path)\n",
        "    train_data[i][1] = extract_name_from_path(path)\n",
        "    i += 1\n",
        "    \n",
        "#print(train_data[0][1])\n",
        "#plt.imshow(train_data[0][0])\n",
        "\n",
        "#sort train_data desc filename to easily match labels\n",
        "train_data.sort(key=lambda x:x[1])\n",
        "\n",
        "#get columns more easy\n",
        "train_data = np.array(train_data)\n",
        "#print(train_data[:,0])\n",
        "\n",
        "label_dict = ['Abdomen', 'Ankle', 'Cervical_Spine', 'Chest', 'Clavicles', 'Elbow', 'Feet', 'Finger', 'Forearm', 'Hand', 'Hip', 'Knee', 'Lower_Leg','Lumbar_Spine','Others', 'Pelvis', 'Shoulder','Sinus', 'Skull','Thigh','Thoracic_Spine','Wrist']\n",
        "\n",
        "import pandas\n",
        "image_label_mapping = pandas.read_csv('../input/unifesp-xray-bodypart-classification/train.csv')\n",
        "\n",
        "#sort image_label_mapping after file name (asc)\n",
        "image_label_mapping.sort_values(by=['SOPInstanceUID'])\n",
        "\n",
        "image_name=image_label_mapping['SOPInstanceUID']\n",
        "temp_labels=np.array(image_label_mapping['Target'])\n",
        "\n",
        "#replace labels with tags\n",
        "labels = []\n",
        "for s in temp_labels:\n",
        "    split = s.split(' ')\n",
        "    for i in range(0,len(split)-1):\n",
        "        if(i < 1):\n",
        "            labels.append(split[i].replace(split[i], label_dict[int(split[i])]))\n",
        "        else:\n",
        "            labels[(i-1)-i] = labels[i] + ' , ' + label_dict[int(split[i])]\n",
        "#print(labels)\n",
        "\n",
        "#Tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(filters=',')\n",
        "\n",
        "tokenizer.fit_on_texts(labels)\n",
        "\n",
        "label_seq = tokenizer.texts_to_sequences(labels)\n",
        "\n",
        "label_length=len(tokenizer.word_index)+1\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "\n",
        "#split into train and test data while combining labels and data\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_data[:,0], labels, test_size=0.2, random_state=1)\n",
        "\n",
        "#print(x_train[0])\n",
        "plt.imshow(x_train[0])\n",
        "print(y_train[0])\n",
        "#print(y_train[0])\n",
        "#print(x_train)\n",
        "#print(y_train)\n",
        "\n",
        "#catergorize\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "labels=[to_categorical(label,num_classes=label_length,dtype='float32').sum(axis=0)[1:] for label in label_seq]\n",
        "\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import VGG19\n",
        "\n",
        "model=models.Sequential()\n",
        "conv_base = VGG19(weights='imagenet',\n",
        "include_top=False,\n",
        "input_shape=input_shape)\n",
        "conv_base.trainable=False\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(22, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "def get_dataset(x,y,batch_size=32):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "steps_per_epoch=int(len(x_train)/32)\n",
        "validation_step=int(len(x_test)/32)\n",
        "\n",
        "train_ds=get_dataset(x_train,np.float32(y_train))\n",
        "test_ds=get_dataset(x_test,np.float32(y_test))\n",
        "\n",
        "history = model.fit(train_ds,\n",
        "                    epochs=50,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_steps=validation_step,   \n",
        "                    validation_data=test_ds)\n",
        "\n",
        "#to decode predictions\n",
        "def tags_mapping(one_hot_encoding):\n",
        "    values = one_hot_encoding.round()\n",
        "    tags = [tokenizer.index_word[i+1] for i in range(len(values)) if values[i] == 1.0]\n",
        "    return tags\n",
        "\n",
        "img_id=range(len(x_train)-1)\n",
        "img,label=x_train[img_id],y_train[img_id]\n",
        "\n",
        "prediction=model.predict(img,steps=1)[0]\n",
        "\n",
        "prediction_tags=tags_mapping(prediction)\n",
        "\n",
        "original_tags=tags_mapping(label)\n",
        "image = x_train[img_id]\n",
        "plt.imshow(image)"
      ]
    }
  ]
}